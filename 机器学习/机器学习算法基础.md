# 机器学习算法基础
## 1.朴素贝叶斯
有以下几个地方需要注意：

1. 如果给出的特征向量长度可能不同，这是需要归一化为通长度的向量（这里以文本分类为例），比如说是句子单词的话，则长度为整个词汇量的长度，对应位置是该单词出现的次数。

2. 计算公式如下：

 $$ p(c_i|w) = \frac{p(w|c_{i})p(c_{i})}{p(w)} $$

 其中一项条件概率可以通过朴素贝叶斯条件独立展开。要注意一点就是$$ p(w|c_i) $$的计算方法，而由朴素贝叶斯的前提假设可知$$ p(w_0,w_1,w_2,w_3...w_n|c_i) $$ = $$ p(w_0|c_i)p(w_1|c_i)p(w_2|c_i)p(w_3|c_i)...p(w_n|c_i)$$,因此一般有两种，一种是在类别为$$c_i$$的那些样本集中，找到$$ w_j $$出现次数的总和，然后除以该样本的总和；第二种方法是类别为$$c_i$$的那些样本集中，找到$$ w_j $$出现次数的总和，然后除以该样本中所有特征出现次数的总和。

3. 如果$$p(w|c_i)$$中的某一项为0，则其联合概率的乘积也可能为0，即2中公式的分子为0，为了避免这种现象出现，一般情况下会将这一项初始化为1，当然为了保证概率相等，分母应对应初始化为2（这里因为是2类，所以加2，如果是k类就需要加k，术语上叫做laplace光滑, 分母加k的原因是使之满足全概率公式）。


- **朴素贝叶斯的优点：**

 对小规模的数据表现很好，适合多分类任务，适合增量式训练。

- **缺点：**

 对输入数据的表达形式很敏感。
